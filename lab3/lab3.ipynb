{"cells":[{"cell_type":"markdown","source":"# Lab 3: The geometry of two views\n\nIn this lab you will learn how to estimate the fundamental matrix that relates two images, corresponding to two different views of the same scene, given a set of correspondences between them. Then, you will apply the estimation of the fundamental matrix to solve the photo-sequencing problem.\n\nThe following file combines some text cells (Markdown cells) and code cells. Some parts of the code need to be completed. All tasks you need to complete are marked in <span style='color:Green'> green.  </span>","metadata":{"cell_id":"00000-383add08-b178-43c6-9d95-3bbe3557affe","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00001-368d3e3e-1f84-448b-b234-81c37e827fe1","deepnote_cell_type":"code"},"source":"import numpy as np\nfrom numpy import linalg as LA\nimport cv2\nimport math\nimport sys\nimport random\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom operator import itemgetter","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## **1. Estimation of the fundamental matrix**\n\n### **1.1 DLT algorithm**\n\nThe first task is to create the function that estimates the fundamental matrix given a set of point correspondences between a pair of images.\n\n<span style='color:Green'> - Complete the function \"fundamental_matrix\" below.  </span>","metadata":{"cell_id":"00002-bff13f69-5c2a-42fc-9e5c-c6b52057c205","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00003-9bc735ea-108b-4621-b1ad-595bf442f843","deepnote_cell_type":"code"},"source":"def fundamental_matrix(points1, points2):\n    \n    # complete ...\n    \n    return F","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"In order to check that the completed function works properly you may use this code which is a toy example where we know the ground truth image.\n\n<span style='color:Green'> - Complete the expression of the ground truth fundamental matrix.  </span>","metadata":{"cell_id":"00004-80120aee-07f3-438d-b02a-e8cced62649b","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00005-e2add949-9949-40c5-8af8-d22020f0806f","deepnote_cell_type":"code"},"source":"# Two camera matrices for testing purposes\nP1 = np.zeros((3,4))\nP1[0,0]=P1[1,1]=P1[2,2]=1\nangle = 15\ntheta = np.radians(angle)\nc = np.cos(theta)\ns = np.sin(theta)\nR = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\nt = np.array([[.3, .1, .2]])\n\nP2 = np.concatenate((R, t.T), axis=1)\nn = 8\nrand = np.random.uniform(0,1,n)\nrand = rand.reshape((1, n))\nrand2 = np.random.uniform(0,1,2*n)\nrand2 = rand2.reshape((2, n))\nones = np.ones((1,n))\nX = np.concatenate((rand2, 3*rand, ones), axis=0)\n\nx1_test = P1 @ X\nx2_test = P2 @ X\n\n# Estimate fundamental matrix (you need to create this function)\nF_es = fundamental_matrix(x1_test, x2_test)\n\n# Ground truth fundamental matrix \nF_gt = # complete ...\n\n# Evaluation: these two matrices should be very similar\nF_gt = np.sign(F_gt[0,0])*F_gt / LA.norm(F_gt)\nF_es = np.sign(F_es[0,0])*F_es / LA.norm(F_es)\nprint(F_gt)\nprint(F_es)\nprint(LA.norm(F_gt-F_es))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.2 Robust estimation of the fundamental matrix**\n\nThe goal of this section is to estimate the fundamental matrix in a practical situation where the image correspondences contain outliers. For that you will have to write the code of the robust version of the previous algorithm.\n\nWe will start by computing and visualizing the image matches.","metadata":{"cell_id":"00006-966127d3-e4f5-4035-8a8e-648a637caa97","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00007-8658f451-c2f9-4f75-8826-fa239f98beb3","deepnote_cell_type":"code"},"source":"# Compute inage correspondences #\n\n# Read images\nimg1 = cv2.imread('Data/0000_s.png',cv2.IMREAD_GRAYSCALE)\nimg2 = cv2.imread('Data/0001_s.png',cv2.IMREAD_GRAYSCALE)\n\n# Initiate ORB detector\norb = cv2.ORB_create(3000)\n# find the keypoints and descriptors with ORB\nkp1, des1 = orb.detectAndCompute(img1,None)\nkp2, des2 = orb.detectAndCompute(img2,None)\n\n# Keypoint matching\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\nmatches = bf.match(des1,des2)\n\n# Show \"good\" matches\nimg_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.imshow(img_12)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style='color:Green'> - Write the function Ransac_fundamental_matrix that embeds in a RANSAC procedure the previous DLT algorithm for estimating the fundamental matrix. You may use the provided RANSAC function in lab 2 as a starting point.  </span>\n\nNote: In order not to end up, eventually, in an infinite loop, it is recommended to set, at each iteration, the maximum number of iterarions to the maximum between a predefined maximum number of iterations by the user and the automatically estimated max. number of iterarions. The estimated number of iterations ensures we pick, with a probability of 0.99, an initial set of correspondences with no outliers.","metadata":{"cell_id":"00008-6c3f5a9b-ec27-404c-8cf6-072373dcdec9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00009-f63983fc-3c02-40bb-97ef-38695097fb68","deepnote_cell_type":"code"},"source":"def Ransac_fundamental_matrix(points1, points2, th, max_it_0):\n    \n    # complete ...\n    \n    return F, inliers","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00010-d4ee8992-b692-4d63-930d-d85b126e3687","deepnote_cell_type":"code"},"source":"# Robust estimation of the fundamental matrix #\npoints1 = []\npoints2 = []\nfor m in matches:\n    points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n    points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1])\n    \npoints1 = np.asarray(points1)\npoints1 = points1.T\npoints2 = np.asarray(points2)\npoints2 = points2.T\n\nF, indices_inlier_matches = Ransac_fundamental_matrix(points1, points2, 1, 5000)\ninlier_matches = itemgetter(*indices_inlier_matches)(matches)\n\nimg_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.imshow(img_12)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.3 Epipolar lines**\n\nNow the fundamental matrix has been mestimated we are going to display some points and their corresponding epipolar lines.\n\n<span style='color:Green'> - Complete the code that computes the epipolar lines in both images.  </span>","metadata":{"cell_id":"00011-57a372dc-8d6d-4b39-ba15-ba3ad9bb706d","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00012-3c4cf0b5-0855-4185-b5f4-11e6be7a28b5","deepnote_cell_type":"code"},"source":"l2 = # epipolar lines in image 2 (complete) \nl1 = # epipolar lines in image 1 (complete) \n\n# choose three random indices\nN = indices_inlier_matches.shape[0]\nindices = random.sample(range(1, N), 3)\n\nm1 = indices_inlier_matches[indices[0]]\nm2 = indices_inlier_matches[indices[1]]\nm3 = indices_inlier_matches[indices[2]]\n\nfrom PIL import Image, ImageDraw\nfrom utils import line_draw, plot_img\n\nimg_path = \"./Data/0000_s.png\"\nI = Image.open(img_path)\nsize = I.size\ncanv = ImageDraw.Draw(I)\nline_draw(l1[:,m1], canv, size)\nline_draw(l1[:,m2], canv, size)\nline_draw(l1[:,m3], canv, size)\ncanv.ellipse((round(points1[0,m1]), round(points1[1,m1]), round(points1[0,m1])+7, round(points1[1,m1])+7), fill = 'red', outline ='red')\ncanv.ellipse((round(points1[0,m2]), round(points1[1,m2]), round(points1[0,m2])+7, round(points1[1,m2])+7), fill = 'red', outline ='red')\ncanv.ellipse((round(points1[0,m3]), round(points1[1,m3]), round(points1[0,m3])+7, round(points1[1,m3])+7), fill = 'red', outline ='red')\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplot_img(I)\n\nimg_path = \"./Data/0001_s.png\"\nI2 = Image.open(img_path)\nsize = I2.size\ncanv2 = ImageDraw.Draw(I2)\nline_draw(l2[:,m1], canv2, size)\nline_draw(l2[:,m2], canv2, size)\nline_draw(l2[:,m3], canv2, size)\ncanv2.ellipse((round(points2[0,m1]), round(points2[1,m1]), round(points2[0,m1])+7, round(points2[1,m1])+7), fill = 'red', outline ='red')\ncanv2.ellipse((round(points2[0,m2]), round(points2[1,m2]), round(points2[0,m2])+7, round(points2[1,m2])+7), fill = 'red', outline ='red')\ncanv2.ellipse((round(points2[0,m3]), round(points2[1,m3]), round(points2[0,m3])+7, round(points2[1,m3])+7), fill = 'red', outline ='red')\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplot_img(I2)\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Application: Photo-sequencing**\n\nIn this part we will compute a simplified version of the algorithm explained in the Photo-sequencing paper [1]. Since we do not have two images taken from roughly the same viewpoint at two different time instants we will manually pick a dynamic point corresponding to a point in a van (identified by index 'idx1') and the projection of its 3D trajectory in the reference image. Then we will compute the projection (to the reference image) of two points on this 3D trajectory at two different time instants (corresponding to the time when the two other provided images where taken). \n\n[1] T. Basha, Y. Moses, and S. Avidan. Photo Sequencing, International Journal of Computer Vision, 110(3), 2014.","metadata":{"cell_id":"00013-7ae5f451-67bf-4baf-a5ff-426926d9105f","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00014-4eadd1d9-332d-4e7f-a997-ff8815e5c3d1","deepnote_cell_type":"code"},"source":"# Compute fundamental matrix between image 1 and 2 #\n\n# Read images\nimg1 = cv2.imread('Data/frame_00000.tif',cv2.IMREAD_GRAYSCALE)\nimg2 = cv2.imread('Data/frame_00001.tif',cv2.IMREAD_GRAYSCALE)\nimg1 = img1[400:, :600]\nimg2 = img2[400:, :600]\n\n# Initiate ORB detector\norb = cv2.ORB_create(nfeatures=4000,scaleFactor =1.1, edgeThreshold = 19, patchSize = 19)\n# find the keypoints and descriptors with ORB\nkp1, des1 = orb.detectAndCompute(img1,None)\nkp2, des2 = orb.detectAndCompute(img2,None)\n        \n# create BFMatcher object\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n# Match descriptors.\nmatches_12 = bf.match(des1,des2)\n\n# Show matches\nimg_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.imshow(img_12)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.show()\n\n# Robust estimation of the fundamental matrix #\npoints1 = []\npoints2 = []\nfor m in matches_12:\n    points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n    points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1])\n    \npoints1 = np.asarray(points1)\npoints1 = points1.T\npoints2 = np.asarray(points2)\npoints2 = points2.T\n\nF, indices_inlier_matches = Ransac_fundamental_matrix(points1, points2, 1, 5000)\ninlier_matches_12 = itemgetter(*indices_inlier_matches)(matches_12)\n\nimg_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.imshow(img_12)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"cell_id":"00015-1734cad9-fce4-433f-8661-482f7de01f57","deepnote_cell_type":"code"},"source":"# Compute fundamental matrix between image 1 and 3 #\n\n# Read images\nimg3 = cv2.imread('Data/frame_00002.tif',cv2.IMREAD_GRAYSCALE)\nimg3 = img3[400:, :600]\n\n# find the keypoints and descriptors with ORB\nkp3, des3 = orb.detectAndCompute(img3,None)\n        \n# create BFMatcher object\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n# Match descriptors.\nmatches_13 = bf.match(des1,des2)\n\n# Show matches \nimg_13 = cv2.drawMatches(img1,kp1,img3,kp3,matches_13,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.imshow(img_13)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.show()\n\n# Robust estimation of the fundamental matrix #\npoints1 = []\npoints3 = []\nfor m in matches_13:\n    points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n    points3.append([kp3[m.trainIdx].pt[0], kp3[m.trainIdx].pt[1], 1])\n    \npoints1 = np.asarray(points1)\npoints1 = points1.T\npoints3 = np.asarray(points3)\npoints3 = points3.T\n\nF_13, indices_inlier_matches_13 = Ransac_fundamental_matrix(points1, points3, 1, 5000)\ninlier_matches_13 = itemgetter(*indices_inlier_matches_13)(matches_13)\n\nimg_13 = cv2.drawMatches(img1,kp1,img3,kp3,inlier_matches_13,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.imshow(img_13)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style='color:Green'> - Complete the code to automatically identify the corresponding point of idx1 in images 2 and 3 (you may use the previously computed matches for that). </span>\n\n<span style='color:Green'> - Complete the code that computes the projection of the van trajectory in image 1. </span>\n\n<span style='color:Green'> - Complete the code that computes the projection of the 3D position of the van in the time instances corresponding to images 2 and 3 (points (x12, y12) and (x13, y13) respectively). </span>","metadata":{"cell_id":"00016-77d7f5f7-998f-4446-9c11-93a0b18419c8","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00017-2682da61-8a81-4955-8d45-50d3ec52998c","deepnote_cell_type":"code"},"source":"idx1 = 123\nidx2 = # complete ...\nidx3 = # complete ...\n           \n# Get the point coordinates\n(x1, y1) = kp1[idx1].pt\n(x2, y2) = kp2[idx2].pt\n(x3, y3) = kp3[idx3].pt\n\npoint1 = np.array([x1, y1, 1]) # 1st point of the trajectory projected in image 1\npoint1_2 = np.array([334, 293, 1]) # 2nd point of the trajectory projected in image 1\ntrajectory = # complete ...\n\nx12, y12 = # complete ...\nx13, y13 = # complete ...\n\nimg_path = \"Data/frame_00000.tif\"\nI = Image.open(img_path).convert('RGB')\nw, h = I.size\nI2 = I.crop((0, 400, 600, h))\ncanv2 = ImageDraw.Draw(I2)\ncanv2.ellipse((x1, y1, x1+7, y1+7), fill = 'yellow', outline ='yellow')\nline_draw(trajectory, canv2, size)\nline_draw(epiline1, canv2, size)\ncanv2.ellipse((x12, y12, x12+7, y12+7), fill = 'cyan', outline ='cyan')\nline_draw(epiline2, canv2, size)\ncanv2.ellipse((x13, y13, x13+7, y13+7), fill = 'blue', outline ='blue')\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nplot_img(I2)            ","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"deepnote_notebook_id":"862e9b9a-5e74-46f6-ab8b-5a7c541b7a38","deepnote_execution_queue":[]}}